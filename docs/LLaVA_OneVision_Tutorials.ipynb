{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Frustratingly Easy) LLaVA OneVision Tutorial\n",
    "\n",
    "We know that it's always beneficial to have a unified interface for different tasks. So we are trying to unify the interface for image, text, image-text interleaved, and video input. And in this tutorial, we aim to provide the most straightforward way to use our model. \n",
    "\n",
    "We use our 0.5B version as an example. This could be running on a GPU with 4GB memory. And with the following examples, you could see it's surprisingly have promising performance on understanding the image, interleaved image-text, and video. Tiny but mighty!\n",
    "\n",
    "The same code could be used for 7B model as well.\n",
    "\n",
    "## Inference Guidance\n",
    "\n",
    "First please install our repo with code and environments: pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git\n",
    "\n",
    "Here is a quick inference code using [lmms-lab/qwen2-0.5b-si](https://huggingface.co/lmms-lab/llava-onevision-qwen2-0.5b-si) as an example. You will need to install `flash-attn` to use this code snippet. If you don't want to install it, you can set `attn_implementation=None` when load_pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Input\n",
    "Tackling the single image input with LLaVA OneVision is pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-si\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "cache_dir = \"/dpc/kunf0097/cache/models\"\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(model_path, None, model_name, device_map=device_map, cache_dir=cache_dir)  # Add any other thing you want to pass in llava_model_args\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image_tensor = process_images([image], image_processor, model.config)\n",
    "image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "\n",
    "conv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\n",
    "question = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size]\n",
    "\n",
    "\n",
    "cont = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensor,\n",
    "    image_sizes=image_sizes,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "print(text_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use the following code to make it streaming in terminal, this would be pretty useful when creating a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "import json\n",
    "\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image_tensor = process_images([image], image_processor, model.config)\n",
    "image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "\n",
    "conv_template = \"qwen_1_5\"\n",
    "question = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size]\n",
    "\n",
    "max_context_length = getattr(model.config, \"max_position_embeddings\", 2048)\n",
    "num_image_tokens = question.count(DEFAULT_IMAGE_TOKEN) * model.get_vision_tower().num_patches\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)\n",
    "\n",
    "max_new_tokens = min(4096, max_context_length - input_ids.shape[-1] - num_image_tokens)\n",
    "\n",
    "if max_new_tokens < 1:\n",
    "    print(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"text\": question + \"Exceeds max token length. Please start a new conversation, thanks.\",\n",
    "                \"error_code\": 0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    gen_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"images\": image_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "    }\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate,\n",
    "        kwargs=dict(\n",
    "            inputs=input_ids,\n",
    "            streamer=streamer,\n",
    "            **gen_kwargs,\n",
    "        ),\n",
    "    )\n",
    "    thread.start()\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        print(generated_text, flush=True)\n",
    "        # print(json.dumps({\"text\": generated_text, \"error_code\": 0}), flush=True)\n",
    "\n",
    "    print(\"Final output:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-Text Interleaved Input\n",
    "\n",
    "Now switching to our onevision model for more complex tasks. You should start to use `llava-onevision-qwen2-0.5b-ov` for image-text interleaved input and video input.\n",
    "\n",
    "Processing image-text interleaved input is a bit more complicated. But following the code below should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "llava_model_args = {\n",
    "        \"multimodal\": True,\n",
    "    }\n",
    "overwrite_config = {}\n",
    "overwrite_config[\"image_aspect_ratio\"] = \"pad\"\n",
    "llava_model_args[\"overwrite_config\"] = overwrite_config\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(model_path, None, model_name, device_map=device_map, **llava_model_args)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Load two images\n",
    "url1 = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "url2 = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\n",
    "\n",
    "image1 = Image.open(requests.get(url1, stream=True).raw)\n",
    "image2 = Image.open(requests.get(url2, stream=True).raw)\n",
    "\n",
    "images = [image1, image2]\n",
    "image_tensors = process_images(images, image_processor, model.config)\n",
    "image_tensors = [_image.to(dtype=torch.float16, device=device) for _image in image_tensors]\n",
    "\n",
    "# Prepare interleaved text-image input\n",
    "conv_template = \"qwen_1_5\"\n",
    "question = f\"{DEFAULT_IMAGE_TOKEN} This is the first image. Can you describe what you see?\\n\\nNow, let's look at another image: {DEFAULT_IMAGE_TOKEN}\\nWhat's the difference between these two images?\"\n",
    "\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size for image in images]\n",
    "\n",
    "# Generate response\n",
    "cont = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensors,\n",
    "    image_sizes=image_sizes,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "print(text_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Input\n",
    "\n",
    "Now let's try video input. It's the same as image input, but you need to pass in a list of video frames. And remember to set the `<image>` token only once in the prompt, e.g. \"<image>\\nWhat is shown in this video?\", not \"<image>\\n<image>\\n<image>\\nWhat is shown in this video?\". Since we trained on this format, it's important to keep the format consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model import *\n",
    "LlavaQwenForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device_map': {'': 0}, 'torch_dtype': torch.float16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: google/siglip-so400m-patch14-384\n",
      "Model Class: LlavaQwenForCausalLM\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model, load_pretrained_model_simplified, connect_parent_lm_head\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX\n",
    "from llava.conversation import conv_templates\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import warnings\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_base =  None# \"amew0/llava-qwen-ov-s2-1016_100248\"\n",
    "model_name = \"llava_qwen\"\n",
    "device_map = {\"\":0}\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\" # \"/dpc/kunf0097/out/checkpoints/lora-llava-qwen-ov-s3-1020_154802\"\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model_simplified(model_path, model_base, model_name, device_map=device_map, attn_implementation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "model_name = \"llava_qwen\"\n",
    "model = connect_parent_lm_head(model, model_path, model_name, device_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = model.lm_head.weight.data.cpu().numpy()\n",
    "b = a[::a.shape[0]//100,::a.shape[1]//100]\n",
    "plt.imshow(b, cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and tokenizer\n",
    "save_path = \"/dpc/kunf0097/.cache/huggingface/hub/llava-qwen-ov-s3-1020_154802\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex =  {\n",
    "        \"id\": \"68fdc346-fa5a-45d2-8309-17269f819213\",\n",
    "        \"video\": \"/dpc/kunf0097/data/hwid12/68fdc346-fa5a-45d2-8309-17269f819213.mp4\",\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"<image>\\nWhat is happening here?\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"Accident Severity: 3 (Motorcycle accidents can be severe due to rider vulnerability.)\\nCongestion Severity: 3 (Possible road blockages causing moderate disruption in traffic.)\\nCause: Motorcycle collisions often occur due to sudden lane changes or poor visibility by the other vehicle.\"\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract frames from video\n",
    "def load_video(video_path, max_frames_num):\n",
    "    if type(video_path) == str:\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    else:\n",
    "        vr = VideoReader(video_path[0], ctx=cpu(0))\n",
    "    total_frame_num = len(vr)\n",
    "    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n",
    "    frame_idx = uniform_sampled_frames.tolist()\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    return spare_frames  # (frames, height, width, channels)\n",
    "\n",
    "\n",
    "# Load and process video\n",
    "video_path = \"/dpc/kunf0097/data/hwid12/68fdc346-fa5a-45d2-8309-17269f819213.mp4\" #ex[\"video\"]\n",
    "video_frames = load_video(video_path, 16)\n",
    "print(video_frames.shape) # (16, 1024, 576, 3)\n",
    "image_tensors = [] \n",
    "frames = image_processor.preprocess(video_frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n",
    "image_tensors.append(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 384, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1024, 576, 3)\n",
      "The!! video!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract frames from video\n",
    "def load_video(video_path, max_frames_num):\n",
    "    if type(video_path) == str:\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    else:\n",
    "        vr = VideoReader(video_path[0], ctx=cpu(0))\n",
    "    total_frame_num = len(vr)\n",
    "    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n",
    "    frame_idx = uniform_sampled_frames.tolist()\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    return spare_frames  # (frames, height, width, channels)\n",
    "\n",
    "\n",
    "# Load and process video\n",
    "video_path = \"docs/jobs.mp4\" #ex[\"video\"]\n",
    "video_frames = load_video(video_path, 16)\n",
    "print(video_frames.shape) # (16, 1024, 576, 3)\n",
    "image_tensors = [] \n",
    "frames = image_processor.preprocess(video_frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n",
    "image_tensors.append(frames)\n",
    "\n",
    "# # Prepare conversation input\n",
    "# conv_template = \"qwen_1_5\"\n",
    "# from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "# question = f\"{DEFAULT_IMAGE_TOKEN}\\nDescribe what's happening in this video.\"\n",
    "\n",
    "# conv = copy.deepcopy(conv_templates[conv_template])\n",
    "# conv.append_message(conv.roles[0], question)\n",
    "# conv.append_message(conv.roles[1], None)\n",
    "# prompt_question = conv.get_prompt()\n",
    "\n",
    "# input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "# image_sizes = [frame.size for frame in video_frames]\n",
    "\n",
    "# # Generate response\n",
    "# cont = model.generate(\n",
    "#     input_ids,\n",
    "#     images=image_tensors,\n",
    "#     image_sizes=image_sizes,\n",
    "#     do_sample=False,\n",
    "#     temperature=0,\n",
    "#     max_new_tokens=4096,\n",
    "#     modalities=[\"video\"],\n",
    "# )\n",
    "# text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "# print(text_outputs[0])\n",
    "\n",
    "# instruction = ex[\"conversations\"][0][\"value\"]\n",
    "# context = ex[\"conversations\"][1][\"value\"]\n",
    "# conv = copy.deepcopy(conv_templates[conv_template])\n",
    "# conv.append_message(conv.roles[0], instruction)\n",
    "# # conv.append_message(conv.roles[0], context)\n",
    "# conv.append_message(conv.roles[1], None)\n",
    "# prompt_question = conv.get_prompt()\n",
    "# print(prompt_question)\n",
    "\n",
    "# input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(model.device)\n",
    "# image_sizes = [frame.size for frame in video_frames]\n",
    "# print(image_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1024, 576, 3)\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<image>\n",
      "\n",
      "Describe what's happening in this video.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "The video features a person standing on a stage, dressed in dark clothing and holding a smartphone. The background is dark with a large hand reaching towards the screen behind them. As the hand reaches out, it appears to be interacting with the phone, which then zooms into the screen displaying an image of a new iPhone. The text overlay reads 'Is this new?' followed by 'this is the new.' This suggests that the person is explaining or demonstrating something related to the new iPhone.\n"
     ]
    }
   ],
   "source": [
    "# from operator import attrgetter\n",
    "# from llava.model.builder import load_pretrained_model\n",
    "# from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "# from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "# from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "# import torch\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# import copy\n",
    "# import warnings\n",
    "# from decord import VideoReader, cpu\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# # Load the OneVision model\n",
    "# pretrained = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# model_name = \"llava_qwen\"\n",
    "# device = \"cuda\"\n",
    "# device_map = \"auto\"\n",
    "# llava_model_args = {\n",
    "#     \"multimodal\": True,\n",
    "#     \"attn_implementation\":None\n",
    "# }\n",
    "# tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map, **llava_model_args)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# Function to extract frames from video\n",
    "def load_video(video_path, max_frames_num):\n",
    "    if type(video_path) == str:\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    else:\n",
    "        vr = VideoReader(video_path[0], ctx=cpu(0))\n",
    "    total_frame_num = len(vr)\n",
    "    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n",
    "    frame_idx = uniform_sampled_frames.tolist()\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    return spare_frames  # (frames, height, width, channels)\n",
    "\n",
    "\n",
    "# Load and process video\n",
    "video_path = \"docs/jobs.mp4\"\n",
    "video_frames = load_video(video_path, 16)\n",
    "print(video_frames.shape) # (16, 1024, 576, 3)\n",
    "image_tensors = []\n",
    "frames = image_processor.preprocess(video_frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n",
    "image_tensors.append(frames)\n",
    "\n",
    "# Prepare conversation input\n",
    "conv_template = \"qwen_1_5\"\n",
    "question = f\"{DEFAULT_IMAGE_TOKEN}\\n\\nDescribe what's happening in this video.\"\n",
    "\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "print(prompt_question)\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [frame.size for frame in video_frames]\n",
    "\n",
    "# Generate response\n",
    "cont = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensors,\n",
    "    image_sizes=image_sizes,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    max_new_tokens=40000,\n",
    "    modalities=[\"video\"],\n",
    ")\n",
    "text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "print(text_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_or_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device_map = {\"\":0}\n",
    "model = AutoModelForCausalLM.from_pretrained(name_or_path, device_map=device_map, dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/dpc/kunf0097/cu-11.8/bin/ptxas\"\n",
    "os.environ[\"TRITON_CUOBJDUMP_PATH\"] = \"/dpc/kunf0097/cu-11.8/bin/cuobjdump\"\n",
    "os.environ[\"TRITON_NVDISASM_PATH\"] = \"/dpc/kunf0097/cu-11.8/bin/nvdisasm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmdeploy import pipeline\n",
    "from lmdeploy.vl import load_image\n",
    "\n",
    "pipe = pipeline('llava-hf/llava-v1.6-mistral-7b-hf')\n",
    "\n",
    "image = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n",
    "response = pipe(('describe this image', image))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_embeddings(paragraph, tokenizer, model):\n",
    "    input_ids = tokenizer(paragraph, return_tensors='pt', truncation=True, padding=True).input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        embeddings = hidden_states[-1]  # Get the last hidden state as embeddings\n",
    "\n",
    "    return embeddings[:, -1, :]  # Get the last token's embedding\n",
    "\n",
    "def compare_expected_and_generated(expected_paragraph, generated_paragraph, tokenizer, model, use_ref=True, reference=\" \"):\n",
    "    expected_embeddings = compute_embeddings(expected_paragraph, tokenizer, model)\n",
    "    generated_embeddings = compute_embeddings(generated_paragraph, tokenizer, model)\n",
    "\n",
    "    cos_exp_gen = torch.nn.functional.cosine_similarity(expected_embeddings, generated_embeddings)\n",
    "    if not use_ref:\n",
    "        return cos_exp_gen.item()\n",
    "    \n",
    "    reference_embeddings = compute_embeddings(reference, tokenizer, model)\n",
    "    cos_ref = torch.nn.functional.cosine_similarity(generated_embeddings, reference_embeddings).to(cos_exp_gen.device)\n",
    "    \n",
    "    cosine_similarity = torch.max((cos_exp_gen - cos_ref)/(1 - cos_ref), torch.tensor(0.0).to(cos_exp_gen.device))\n",
    "    \n",
    "    return cosine_similarity.item(z)\n",
    "\n",
    "# Example usage\n",
    "expected_paragraph = \"Hey how are you doing?\"\n",
    "generated_paragraph = \"Are you ok .\"\n",
    "cs = compare_expected_and_generated(expected_paragraph, generated_paragraph, tokenizer, model, use_ref=False)\n",
    "\n",
    "print(f\"Similarity Loss: {cs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python similarity.py out/Qwen2-7B-Instruct/s1_test_llava-qwen-ov-s1-1015_215421.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# # s1\n",
    "# classes = ['Base', 'S1_1e', 'S1_10e']\n",
    "# data = [\".7723±.1972\", \".90688±.067088\", \".889768±.0743\"]\n",
    "\n",
    "# s2\n",
    "# classes = ['Base', 'S1_1e', 'S1_10e', 'S2_1e', 'S2_10e']\n",
    "# data = [\".7382±.1638\", \".8727±.0534\", \".88139±.0743\", \".9037±.0386\", \".90348±.06081\"]\n",
    "\n",
    "# s3\n",
    "classes = [\"S3_1e\", \"S3_10e(base S2_1e)\", \"S3_10e\"]\n",
    "data = [\".86385±.0662\", \".85234±.08173\", \".879±.0652\"]\n",
    "\n",
    "# Extracting accuracy and standard deviation from the string\n",
    "accuracies = [float(d.split('±')[0]) for d in data]\n",
    "std_devs = [float(d.split('±')[1]) for d in data]\n",
    "\n",
    "# Plotting the graph\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(classes))\n",
    "\n",
    "ax.errorbar(\n",
    "    x, accuracies, yerr=std_devs, fmt='o', color='red',\n",
    "    ecolor='black', elinewidth=1, capsize=5, capthick=1\n",
    ")\n",
    "for acc in accuracies:\n",
    "    ax.hlines(y=acc, xmin=0, xmax=len(classes)-1, colors='gray', linestyles='dashed', linewidth=1.5)\n",
    "\n",
    "# Customizing the plot\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_ylim(0.76, 1)  # Extend y-axis slightly above 1 for better visibility\n",
    "# ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Qwen2 Embedded Cosine Similarity')\n",
    "ax.set_title('Data: Input (A) (S3)')\n",
    "# plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(model, input_data=input_ids)\n",
    "\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kunet.ae/ku5001069/LLaVA-NeXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd LLaVA-NeXT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/3replicas_2gpu.json\") as f:\n",
    "    data = json.load(f)[\"requests\"]\n",
    "with open(\"out/6replicas_033gpu.json\") as f6:\n",
    "    data6 = json.load(f6)[\"requests\"]\n",
    "with open(\"out/9replicas_16_9gpu.json\") as f9:\n",
    "    data9 = json.load(f9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Extracting values\n",
    "x = [d[\"total_requests\"] for d in data]\n",
    "x6 = [d[\"total_requests\"] for d in data6]\n",
    "# x9 = [d[\"total_requests\"] for d in data9]\n",
    "\n",
    "y = [d[\"average_latency\"] for d in data]\n",
    "y6 = [d[\"average_latency\"] for d in data6]\n",
    "# y9 = [d[\"average_latency\"] for d in data9]\n",
    "\n",
    "yerr = [\n",
    "    [y_i - d[\"min_latency\"] for y_i, d in zip(y, data)],  # Lower error\n",
    "    [d[\"max_latency\"] - y_i for y_i, d in zip(y, data)]   # Upper error\n",
    "]\n",
    "yerr6 = [\n",
    "    [y_i - d[\"min_latency\"] for y_i, d in zip(y6, data6)],  # Lower error\n",
    "    [d[\"max_latency\"] - y_i for y_i, d in zip(y6, data6)]   # Upper error\n",
    "]\n",
    "# yerr9 = [\n",
    "#     [y_i - d[\"min_latency\"] for y_i, d in zip(y9, data9)],  # Lower error\n",
    "#     [d[\"max_latency\"] - y_i for y_i, d in zip(y9, data9)]   # Upper error\n",
    "# ]\n",
    "\n",
    "# Plotting\n",
    "plt.errorbar(x, y, yerr=yerr, fmt='o', color='red', ecolor='red', capsize=5, label='3 replicas')\n",
    "plt.errorbar(x6, y6, yerr=yerr6, fmt='o', color='blue', ecolor='blue', capsize=5, label='6 replicas')\n",
    "# plt.errorbar(x9, y9, yerr=yerr9, fmt='o', color='green', ecolor='green', capsize=5, label='9 replicas')\n",
    "\n",
    "# plt.plot(x, y, color='red', linestyle='-', linewidth=2)\n",
    "# plt.plot(x6, y6, color='blue', linestyle='-', linewidth=2)\n",
    "# plt.plot(x9, y9, color='green', linestyle='-', linewidth=2)\n",
    "# Adding labels and title\n",
    "plt.xlabel('Requests')\n",
    "plt.ylabel('Response Time')\n",
    "plt.title('Response Time')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
