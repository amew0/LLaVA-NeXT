{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"4\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"]=\"0\"\n",
    "os.environ[\"NCCL_IB_GID_INDEX\"]=\"3\"\n",
    "os.environ[\"NCCL_SOCKET_IFNAME\"]=\"bond0\" #$(ifconfig | awk '/^[a-z]/ {gsub(/:/, \"\"); print $1; exit}')\n",
    "os.environ[\"NCCL_DEBUG\"]=\"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-04 16:26:33,256] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from PIL import Image, ImageFile\n",
    "from packaging import version\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "import math\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "import deepspeed\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from torch.utils.data import Dataset\n",
    "from llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.mm_utils import process_highres_image, process_anyres_image, process_highres_image_crop_split, tokenizer_image_token\n",
    "from llava.utils import rank0_print, process_video_with_pyav, process_video_with_decord\n",
    "import torch.distributed as dist\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "local_rank = None\n",
    "\n",
    "IS_TOKENIZER_GREATER_THAN_0_14 = version.parse(tokenizers.__version__) >= version.parse(\"0.14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"MASTER_ADDR\"] = \"172.18.10.4\"\n",
    "# os.environ[\"MASTER_PORT\"] = \"29544\"\n",
    "# dist.init_process_group(backend='nccl', rank=0, world_size=2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from llava.train.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"parser.pkl\", \"rb\") as f:\n",
    "    p = pickle.load(f)\n",
    "    model_args = p['model_args']\n",
    "    data_args = p['data_args']\n",
    "    training_args = p['training_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.eval_data_path = \"/home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_test_v2.json\"\n",
    "# vars(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!Inside train function!!\n",
      "Inspecting experiment hyperparameters:\n",
      "\n",
      "model_args = {'model_name_or_path': '/dpc/kunf0097/.cache/huggingface/hub/v2-llava-qwen-ov-s2-1028_182909', 'model_class_name': None, 'mm_tunable_parts': 'mm_vision_tower,mm_mlp_adapter,mm_language_model', 'version': 'qwen_1_5', 'freeze_backbone': False, 'tune_mm_mlp_adapter': False, 'tune_mm_vision_resampler': False, 'vision_tower': 'google/siglip-so400m-patch14-384', 'vision_tower_pretrained': None, 'unfreeze_mm_vision_tower': False, 'unfreeze_language_model': False, 'mm_vision_select_layer': -2, 'pretrain_mm_mlp_adapter': None, 'mm_projector_type': 'mlp2x_gelu', 'mm_use_im_start_end': False, 'mm_use_im_patch_token': False, 'mm_patch_merge_type': 'spatial_unpad', 'mm_vision_select_feature': 'patch', 'mm_resampler_type': None, 'mm_mask_drop_mode': 'fixed', 'mm_mask_drop_skip_percentage': 0.0, 'mm_mask_drop_ratio': 0.25, 'mm_mask_drop_ratio_upper': None, 'mm_mask_drop_ratio_lower': None, 'mm_spatial_pool_stride': None, 'mm_spatial_pool_mode': 'bilinear', 'mm_spatial_pool_out_channels': None, 'mm_perceiver_depth': 3, 'mm_perceiver_latents': 32, 'mm_perceiver_ff_mult': 4, 'mm_perceiver_pretrained': None, 'mm_qformer_depth': 3, 'mm_qformer_latents': 32, 'mm_qformer_pretrained': None, 'rope_scaling_factor': None, 'rope_scaling_type': None, 's2': False, 's2_scales': '336,672,1008', 'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_newline_position': 'grid', 'delay_load': True, 'add_faster_video': False, 'faster_token_stride': 10}\n",
      "\n",
      "\n",
      "data_args = {'data_path': '/home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_train_v2.json', 'lazy_preprocess': True, 'is_multimodal': True, 'early_mix_text': False, 'image_folder': None, 'image_aspect_ratio': 'anyres_max_9', 'image_grid_pinpoints': [[384, 384], [384, 768], [384, 1152], [384, 1536], [384, 1920], [384, 2304], [768, 384], [768, 768], [768, 1152], [768, 1536], [768, 1920], [768, 2304], [1152, 384], [1152, 768], [1152, 1152], [1152, 1536], [1152, 1920], [1152, 2304], [1536, 384], [1536, 768], [1536, 1152], [1536, 1536], [1536, 1920], [1536, 2304], [1920, 384], [1920, 768], [1920, 1152], [1920, 1536], [1920, 1920], [1920, 2304], [2304, 384], [2304, 768], [2304, 1152], [2304, 1536], [2304, 1920], [2304, 2304]], 'image_crop_resolution': None, 'image_split_resolution': None, 'video_folder': None, 'video_fps': 1, 'frames_upbound': 32, 'add_time_instruction': False, 'force_sample': False, 'image_processor': <llava.model.multimodal_encoder.siglip_encoder.SigLipImageProcessor object at 0x14927b860d30>, 'mm_use_im_start_end': False, 'dataset_paths': ['/home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_train_v2.json'], 'eval_data_path': '/home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_test_v2.json'}\n",
      "\n",
      "\n",
      "training_args = {'output_dir': '/dpc/kunf0097/out/checkpoints/v2-lora-llava-qwen-ov-s3-1104_152911', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': True, 'do_predict': False, 'evaluation_strategy': <IntervalStrategy.STEPS: 'steps'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 2, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 1e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 10.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.03, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/dpc/kunf0097/out/checkpoints/v2-lora-llava-qwen-ov-s3-1104_152911/runs/Nov04_15-29-36_gpu-10-4', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 1.0, 'logging_nan_inf_filter': True, 'save_strategy': <IntervalStrategy.STEPS: 'steps'>, 'save_steps': 0.1, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': True, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': False, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': True, 'eval_steps': 1.0, 'dataloader_num_workers': 1, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'v2-lora-llava-qwen-ov-s3-1104_152911', 'disable_tqdm': False, 'remove_unused_columns': False, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, gradient_accumulation_kwargs=None), 'deepspeed': 'scripts/zero3.json', 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': [], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': False, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': True, 'torch_compile_backend': 'inductor', 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'cache_dir': '/dpc/kunf0097/cache/models', 'freeze_mm_mlp_adapter': False, 'freeze_mm_vision_resampler': False, 'mpt_attn_impl': 'triton', 'model_max_length': 32768, 'double_quant': True, 'quant_type': 'nf4', 'bits': 16, 'lora_enable': True, 'lora_r': 16, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_weight_path': '', 'lora_bias': 'none', 'mm_projector_lr': None, 'mm_vision_tower_lr': 2e-06, 'group_by_varlen': False, 'group_by_modality_length': True, 'group_by_modality_length_auto': False, 'verbose_logging': True, 'attn_implementation': 'flash_attention_2', 'distributed_state': Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 2\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      ", '_n_gpu': 1, '__cached__setup_devices': device(type='cuda', index=0), 'deepspeed_plugin': DeepSpeedPlugin(hf_ds_config=<transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x14927b860d00>, gradient_accumulation_steps=2, gradient_clipping=1.0, zero_stage=3, is_train_batch_min=True, offload_optimizer_device='none', offload_param_device='none', offload_optimizer_nvme_path='none', offload_param_nvme_path='none', zero3_init_flag=True, zero3_save_16bit_model=True, transformer_moe_cls_names=None), 'hf_deepspeed_config': <transformers.integrations.deepspeed.HfTrainerDeepSpeedConfig object at 0x14927b860d00>, 'mm_tunable_parts': 'mm_vision_tower,mm_mlp_adapter,mm_language_model', 'use_im_start_end': False}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"!!Inside train function!!\")\n",
    "global local_rank\n",
    "\n",
    "# parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "# model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "if training_args.verbose_logging:\n",
    "    rank0_print(f\"Inspecting experiment hyperparameters:\\n\")\n",
    "    rank0_print(f\"model_args = {vars(model_args)}\\n\\n\")\n",
    "    rank0_print(f\"data_args = {vars(data_args)}\\n\\n\")\n",
    "    rank0_print(f\"training_args = {vars(training_args)}\\n\\n\")\n",
    "    # rank0_print(f\"evaluation_args = {vars(evaluation_args)}\\n\\n\")\n",
    "\n",
    "local_rank = training_args.local_rank\n",
    "compute_dtype = torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32)\n",
    "\n",
    "bnb_model_from_pretrained_args = {}\n",
    "if training_args.bits in [4, 8]:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    bnb_model_from_pretrained_args.update(\n",
    "        dict(\n",
    "            device_map={\"\": training_args.device},\n",
    "            load_in_4bit=training_args.bits == 4,\n",
    "            load_in_8bit=training_args.bits == 8,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=training_args.bits == 4,\n",
    "                load_in_8bit=training_args.bits == 8,\n",
    "                llm_int8_threshold=6.0,\n",
    "                llm_int8_has_fp16_weight=False,\n",
    "                bnb_4bit_compute_dtype=compute_dtype,\n",
    "                bnb_4bit_use_double_quant=training_args.double_quant,\n",
    "                bnb_4bit_quant_type=training_args.quant_type,  # {'fp4', 'nf4'}\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "    metric = evaluate.load('bertscore')\n",
    "\n",
    "    class BertScoreMetric:\n",
    "        def __init__(self):\n",
    "            self.precision_scores = []\n",
    "            self.recall_scores = []\n",
    "            self.f1_scores = []\n",
    "\n",
    "        def update(self, predicted_deto, label_deto):\n",
    "            # Calculate scores for this batch\n",
    "            score = metric.compute(predictions=predicted_deto, references=label_deto, model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "            \n",
    "            # Store batch-level scores\n",
    "            self.precision_scores.extend(score['precision'])\n",
    "            self.recall_scores.extend(score['recall'])\n",
    "            self.f1_scores.extend(score['f1'])\n",
    "\n",
    "        def compute(self):\n",
    "            # Aggregate scores across batches\n",
    "            mean = lambda l: sum(l) / len(l)\n",
    "            result = {\n",
    "                \"precision\": mean(self.precision_scores),\n",
    "                \"recall\": mean(self.recall_scores),\n",
    "                \"f1\": mean(self.f1_scores)\n",
    "            }\n",
    "            \n",
    "            # Reset batch statistics\n",
    "            self.precision_scores = []\n",
    "            self.recall_scores = []\n",
    "            self.f1_scores = []\n",
    "            \n",
    "            return result\n",
    "\n",
    "    # Instantiate your custom metric\n",
    "    bert_score_metric = BertScoreMetric()\n",
    "\n",
    "    def compute_metrics(eval_pred, compute_result=True):\n",
    "        rank0_print(f\"Computing metrics... {compute_result}\")\n",
    "        \n",
    "        # Extract predictions and labels\n",
    "        predictions = eval_pred.predictions[0]  # Shape: (2, 201, 15200)\n",
    "        predicted_classes = torch.argmax(predictions, dim=-1)  # Shape: (2, 201)\n",
    "        labels = eval_pred.label_ids  # Shape: (2, 201)\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        predicted_deto = tokenizer.batch_decode(predicted_classes)\n",
    "        label_deto = tokenizer.batch_decode(label[label > -1] for label in labels)\n",
    "        \n",
    "        # Update metric with this batch's decoded results\n",
    "        bert_score_metric.update(predicted_deto, label_deto)\n",
    "        \n",
    "        if compute_result:\n",
    "            # Aggregate and return final metrics\n",
    "            return bert_score_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'mm_spatial_pool_mode': 'bilinear'}\n",
      "Loading vision tower: google/siglip-so400m-patch14-384\n",
      "Adding LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt version: qwen_1_5\n"
     ]
    }
   ],
   "source": [
    "model = get_model(model_args, training_args, bnb_model_from_pretrained_args)\n",
    "model.config.use_cache = False\n",
    "if model_args.rope_scaling_factor is not None and model_args.rope_scaling_type is not None:\n",
    "    model.config.rope_scaling = {\n",
    "        \"factor\": model_args.rope_scaling_factor,\n",
    "        \"type\": model_args.rope_scaling_type,\n",
    "    }\n",
    "\n",
    "if model_args.freeze_backbone:\n",
    "    model.model.requires_grad_(False)\n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "\n",
    "    model.config.torch_dtype = torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32)\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "if training_args.lora_enable:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=training_args.lora_r,\n",
    "        lora_alpha=training_args.lora_alpha,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        lora_dropout=training_args.lora_dropout,\n",
    "        bias=training_args.lora_bias,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    if training_args.bits == 16:\n",
    "        if training_args.bf16:\n",
    "            model.to(torch.bfloat16)\n",
    "        if training_args.fp16:\n",
    "            model.to(torch.float16)\n",
    "    rank0_print(\"Adding LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "if \"mistral\" in model_args.model_name_or_path.lower() or \"mixtral\" in model_args.model_name_or_path.lower() or \"zephyr\" in model_args.model_name_or_path.lower():\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=training_args.cache_dir, model_max_length=training_args.model_max_length, padding_side=\"left\")\n",
    "elif \"qwen\" in model_args.model_name_or_path.lower():\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=training_args.cache_dir, model_max_length=training_args.model_max_length, padding_side=\"right\")\n",
    "elif (\n",
    "    \"wizardlm-2\" in model_args.model_name_or_path.lower()\n",
    "    or \"vicuna\" in model_args.model_name_or_path.lower()\n",
    "    or \"llama\" in model_args.model_name_or_path.lower()\n",
    "    or \"yi\" in model_args.model_name_or_path.lower()\n",
    "    or \"nous-hermes\" in model_args.model_name_or_path.lower()\n",
    "    and \"wizard-2\" in model_args.model_name_or_path.lower()\n",
    "):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "rank0_print(f\"Prompt version: {model_args.version}\")\n",
    "if model_args.version == \"v0\":\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=\"[PAD]\"),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "elif model_args.version == \"v0.5\":\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "else:\n",
    "    if tokenizer.unk_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "    if model_args.version in conversation_lib.conv_templates:\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\n",
    "    else:\n",
    "        conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/siglip-so400m-patch14-384 is already loaded, `load_model` called again, skipping.\n",
      "Using mm_tunable_parts: mm_vision_tower,mm_mlp_adapter,mm_language_model\n",
      "Total parameters: ~1040.91 MB)\n",
      "Trainable parameters: ~1040.91 MB)\n"
     ]
    }
   ],
   "source": [
    "if model_args.vision_tower is not None:\n",
    "    model.get_model().initialize_vision_modules(model_args=model_args, fsdp=training_args.fsdp)\n",
    "\n",
    "    vision_tower = model.get_vision_tower()\n",
    "    vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "    data_args.image_processor = vision_tower.image_processor\n",
    "    data_args.is_multimodal = True\n",
    "\n",
    "    model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "    if data_args.image_grid_pinpoints is not None:\n",
    "        if isinstance(data_args.image_grid_pinpoints, str) and \"x\" in data_args.image_grid_pinpoints:\n",
    "            try:\n",
    "                patch_size = data_args.image_processor.size[0]\n",
    "            except Exception as e:\n",
    "                patch_size = data_args.image_processor.size[\"shortest_edge\"]\n",
    "\n",
    "            assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n",
    "            # Use regex to extract the range from the input string\n",
    "            matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", data_args.image_grid_pinpoints)\n",
    "            range_start = tuple(map(int, matches[0]))\n",
    "            range_end = tuple(map(int, matches[-1]))\n",
    "            # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n",
    "            grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n",
    "            # Multiply all elements by patch_size\n",
    "            data_args.image_grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n",
    "        elif isinstance(data_args.image_grid_pinpoints, str):\n",
    "            data_args.image_grid_pinpoints = ast.literal_eval(data_args.image_grid_pinpoints)\n",
    "\n",
    "    model.config.image_grid_pinpoints = data_args.image_grid_pinpoints\n",
    "    model.config.image_crop_resolution = data_args.image_crop_resolution\n",
    "    model.config.image_split_resolution = data_args.image_split_resolution\n",
    "    model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "    model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "    model.config.mm_newline_position = model_args.mm_newline_position\n",
    "    model.config.add_faster_video = model_args.add_faster_video\n",
    "    model.config.faster_token_stride = model_args.faster_token_stride\n",
    "    model.config.add_time_instruction = data_args.add_time_instruction\n",
    "    model.config.force_sample = data_args.force_sample\n",
    "    model.config.mm_spatial_pool_stride = model_args.mm_spatial_pool_stride \n",
    "\n",
    "    ### Deciding train which part of the model\n",
    "    if model_args.mm_tunable_parts is None:  # traditional way of deciding which part to train\n",
    "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "        model.config.tune_mm_vision_resampler = training_args.tune_mm_vision_resampler = model_args.tune_mm_vision_resampler\n",
    "        if model_args.tune_mm_mlp_adapter or model_args.tune_mm_vision_resampler:\n",
    "            model.requires_grad_(False)\n",
    "        if model_args.tune_mm_mlp_adapter:\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "        if model_args.tune_mm_vision_resampler:\n",
    "            for p in model.get_model().vision_resampler.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "        if training_args.freeze_mm_mlp_adapter:\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        model.config.freeze_mm_vision_resampler = training_args.freeze_mm_vision_resampler\n",
    "        if training_args.freeze_mm_vision_resampler:\n",
    "            for p in model.get_model().vision_resampler.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        model.config.unfreeze_mm_vision_tower = model_args.unfreeze_mm_vision_tower\n",
    "        if model_args.unfreeze_mm_vision_tower:\n",
    "            vision_tower.requires_grad_(True)\n",
    "        else:\n",
    "            vision_tower.requires_grad_(False)\n",
    "\n",
    "    else:\n",
    "        rank0_print(f\"Using mm_tunable_parts: {model_args.mm_tunable_parts}\")\n",
    "        model.config.mm_tunable_parts = training_args.mm_tunable_parts = model_args.mm_tunable_parts\n",
    "        # Set the entire model to not require gradients by default\n",
    "        model.requires_grad_(False)\n",
    "        vision_tower.requires_grad_(False)\n",
    "        model.get_model().mm_projector.requires_grad_(False)\n",
    "        model.get_model().vision_resampler.requires_grad_(False)\n",
    "        # Parse the mm_tunable_parts to decide which parts to unfreeze\n",
    "        tunable_parts = model_args.mm_tunable_parts.split(\",\")\n",
    "        if \"mm_mlp_adapter\" in tunable_parts:\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "        if \"mm_vision_resampler\" in tunable_parts:\n",
    "            for p in model.get_model().vision_resampler.parameters():\n",
    "                p.requires_grad = True\n",
    "        if \"mm_vision_tower\" in tunable_parts:\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"vision_tower\" in name:\n",
    "                    param.requires_grad_(True)\n",
    "        if \"mm_language_model\" in tunable_parts:\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"vision_tower\" not in name and \"mm_projector\" not in name and \"vision_resampler\" not in name:\n",
    "                    param.requires_grad_(True)\n",
    "\n",
    "    total_params = sum(p.ds_numel if hasattr(p, \"ds_numel\") else p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.ds_numel if hasattr(p, \"ds_numel\") else p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    rank0_print(f\"Total parameters: ~{total_params/1e6:.2f} MB)\")\n",
    "    rank0_print(f\"Trainable parameters: ~{trainable_params/1e6:.2f} MB)\")\n",
    "    if training_args.bits in [4, 8]:\n",
    "        model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)\n",
    "\n",
    "    model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "    model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "    model.config.mm_vision_tower_lr = training_args.mm_vision_tower_lr\n",
    "    training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "    model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "    model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.bits in [4, 8]:\n",
    "    from peft.tuners.lora import LoraLayer\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer):\n",
    "            if training_args.bf16:\n",
    "                module = module.to(torch.bfloat16)\n",
    "        if \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "            if hasattr(module, \"weight\"):\n",
    "                if training_args.bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_train_v2.json\n",
      "Loaded 1525 samples from /home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_train_v2.json\n",
      "Loaded 1525 samples from /home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_train_v2.json\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Loading /home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_test_v2.json\n",
      "Loaded 382 samples from /home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_test_v2.json\n",
      "Loaded 382 samples from /home/kunet.ae/ku5001069/LLaVA-NeXT/data/s3/s3_test_v2.json\n",
      "Formatting inputs...Skip in lazy mode\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting NCCL timeout to INF to avoid running errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='7620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/7620 : < :, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/transformers/trainer.py:2278\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/transformers/trainer.py:2644\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2641\u001b[0m logs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;66;03m# all_gather + mean() to get average loss over all processes\u001b[39;00m\n\u001b[0;32m-> 2644\u001b[0m tr_loss_scalar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nested_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;66;03m# reset tr_loss to zero\u001b[39;00m\n\u001b[1;32m   2647\u001b[0m tr_loss \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/transformers/trainer.py:3756\u001b[0m, in \u001b[0;36mTrainer._nested_gather\u001b[0;34m(self, tensors, name)\u001b[0m\n\u001b[1;32m   3752\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m smp_gather(tensors)\n\u001b[1;32m   3753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdistributed_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3755\u001b[0m ):\n\u001b[0;32m-> 3756\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[43mdistributed_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:220\u001b[0m, in \u001b[0;36mdistributed_concat\u001b[0;34m(tensor, num_total_examples)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)({k: distributed_concat(t, num_total_examples) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m    219\u001b[0m tensor \u001b[38;5;241m=\u001b[39m atleast_1d(tensor)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 220\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m [tensor\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)]\n\u001b[1;32m    221\u001b[0m dist\u001b[38;5;241m.\u001b[39mall_gather(output_tensors, tensor)\n\u001b[1;32m    222\u001b[0m concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(output_tensors, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:1196\u001b[0m, in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:576\u001b[0m, in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03mHelper that gets a given group's world size.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 576\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/dpc/kunf0097/.conda/envs/llavanext/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:707\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m     )\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('bertscore')\n",
    "def compute_metrics(eval_pred, compute_result=True):  \n",
    "    predictions = eval_pred.predictions[0]  # Shape: (2, 201, 15200)\n",
    "    \n",
    "    predicted_classes = torch.argmax(predictions, dim=-1)  # Shape: (2, 201)\n",
    "    labels = eval_pred.label_ids             # Shape: (2, 201)\n",
    "    \n",
    "    predicted_deto = tokenizer.batch_decode(predicted_classes)\n",
    "    label_deto = tokenizer.batch_decode(label[label > -1] for label in labels)\n",
    "    \n",
    "    print(predicted_deto)\n",
    "    print(label_deto)\n",
    "    \n",
    "    score = metric.compute(predictions=predicted_deto, references=label_deto, model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "    mean = lambda l: sum(l) / len(l)\n",
    "    print(score)\n",
    "    # a = 2/0\n",
    "    return {'precision': mean(score['precision']), 'recall': mean(score['recall']), 'f1': mean(score['f1'])}\n",
    "\n",
    "training_args.report_to = []\n",
    "\n",
    "trainer = LLaVATrainer(model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        args=training_args,\n",
    "                        compute_metrics=compute_metrics, \n",
    "                        **data_module)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "if training_args.lora_enable:\n",
    "    state_dict = get_peft_state_maybe_zero_3(model.named_parameters(), training_args.lora_bias)\n",
    "    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(model.named_parameters())\n",
    "    if training_args.local_rank == 0 or training_args.local_rank == -1:\n",
    "        if hasattr(model, \"config\"):\n",
    "            model.config.save_pretrained(training_args.output_dir)\n",
    "        if hasattr(model, \"generation_config\"):\n",
    "            model.generation_config.save_pretrained(training_args.output_dir)\n",
    "        model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n",
    "        torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, \"non_lora_trainables.bin\"))\n",
    "else:\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "\n",
    "rank0_print(f\"Model saved to {training_args.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
